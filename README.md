# ğŸš€ End-to-End PySpark ETL Pipeline

This project demonstrates a **complete ETL (Extract, Transform, Load)** workflow built using **Apache PySpark**.  
The pipeline performs **data ingestion from CSV**, applies **transformations and aggregations**, and generates **cleaned insights** ready for analytics and reporting.


## ğŸ“ Project Overview

The **End-to-End PySpark ETL Pipeline** is designed to automate the process of reading raw CSV data, transforming it using PySpark, and producing high-quality structured datasets for downstream analytics.

This project showcases real-time, production-style ETL logic, and demonstrates how to work with distributed data efficiently using PySpark in a **free-tier, local environment (MacOS)**.


## ğŸ§© Architecture

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚        Raw Data (CSV)      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Extract (PySpark)   â”‚
                  â”‚  - Read CSV          â”‚
                  â”‚  - Schema Validation â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Transform (PySpark)  â”‚
                  â”‚ - Data Cleaning      â”‚
                  â”‚ - Aggregation        â”‚
                  â”‚ - Business Logic     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Load (PySpark)     â”‚
                  â”‚ - Write to Output    â”‚
                  â”‚ - Save as Parquet/CSVâ”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Tech Stack:

| Component   | Tool/Version         |
| ----------- | -------------------- |
| Language    | Python 3.11          |
| Framework   | Apache PySpark 4.0.1 |
| OS          | macOS                |
| IDE         | VS Code / PyCharm    |
| Data Source | CSV Files            |

âš™ï¸ Features

âœ… End-to-End ETL Workflow (Extract â†’ Transform â†’ Load)
âœ… Schema validation and null handling
âœ… Aggregation and transformation logic using PySpark
âœ… Modular, reusable Python code structure
âœ… Runs entirely on free-tier (local) environment

ğŸ“Š Output

After execution, the pipeline generates cleansed and aggregated datasets, which can be easily connected to Power BI, Tableau, or any BI tool for visualization.

ğŸ’¡ What I Learned

Building scalable ETL pipelines using Apache PySpark

Handling schema validation, joins, and transformations

Managing file I/O operations on macOS

Structuring Python code for modular and reusable data workflows

Setting up Git and pushing a project from local machine to GitHub